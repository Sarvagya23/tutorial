# -*- coding: utf-8 -*-
"""mnist-classification-with-lenet-5-architecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-lkb6g7AveYINvygnoCIxRLXcEpSKZVW

# MNIST Classification with LeNet-5 Architecture

LeNet-5 was created in 1998 by Yann Lacun, and was pioneering in its time for the performace.  As the -5 in the name suggests, the architecture consists of 5 hidden layers; 3 convolutional layers and 2 fully connected layers.  Two other differences are that it uses tanh as an activation function and average pooling, which have largely been replaced by ReLU and max pooling in current architectures.  Although nearly 25 years old, it may be of interest to others studying neural networks due to its historical significance.

This notebook implements the LeNet-5 architecture on the MNIST dataset using Keras.  Portions of the code in this notebook are adapted from the notebook [here](https://github.com/moelgendy/deep_learning_for_vision_systems/blob/master/chapter_05/lenet_implementation_in_keras.ipynb). 

The original paper is [here](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

## Imports
"""

import numpy as np 
import pandas as pd 
pd.set_option('display.max_columns', 50)

import matplotlib.pyplot as plt 
import plotly.express as px
import seaborn as sns
sns.set_style('darkgrid')

import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense
from keras.preprocessing.image import ImageDataGenerator

import warnings
warnings.filterwarnings("ignore")

"""## Load Data and Top Level EDA"""

train = pd.read_csv('app/train.csv') 
test = pd.read_csv('app/test.csv') 
ss = pd.read_csv('app/sample_submission.csv')

print(f'Training Data Shape: {train.shape}') 
print(f'Test Data Shape: {test.shape}') 
print(f'Sample Submission Shape: {ss.shape}') 


y = train['label']
train = train.iloc[:, 1:]

y.value_counts()

"""## Reshape array for CNN"""

# -1 means that it is an unknown dimension and numpy should figure it out from other dimensions.
train = train.values.reshape(-1,28,28,1)
test = test.values.reshape(-1,28,28,1)
test.shape

"""## One hot encode y variable"""

# one hot encode y variable.
y_ohe = tf.keras.utils.to_categorical(y, num_classes=10)
y_ohe.shape

"""## Look at a few samples"""

# plot first six training images
fig = plt.figure(figsize=(20,20))
for i in range(6):
    ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])
    ax.imshow(train[i], cmap='gray')
    ax.set_title(str(y[i]))

"""## Train Test Split"""

from sklearn.model_selection import train_test_split
x_train, x_valid, y_train, y_valid = train_test_split(train, y_ohe, train_size=0.8, test_size=0.2,
                                                      random_state=12)
x_test = test

"""## LeNet-5 Model"""

# LeNet-5 Architecture
model = Sequential()

model.add(Conv2D(filters=6,kernel_size=5, strides=1,activation='tanh',input_shape=(28,28,1),padding='same'))

model.add(AveragePooling2D(pool_size=2, strides=2,padding='valid'))

model.add(Conv2D(filters=16,kernel_size=5, strides=1,activation='tanh',padding='valid'))

model.add(AveragePooling2D(pool_size=2, strides=2,padding='valid'))

model.add(Conv2D(filters=120,kernel_size=5, strides=1,activation='tanh',padding='valid'))

model.add(Flatten())

model.add(Dense(units=84, activation='tanh'))

model.add(Dense(units=10,activation='softmax'))

model.summary()

# use categorical cross entropy loss function when have multiple classes
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint, LearningRateScheduler

# set the learning rate decay as utilized in the original 1998 paper: 
#    .00005 for the first 2 layers 
#    .0002 for the next 3
#    .000005 for the next 4
#    .000001 for the rest

def lr_schedule(epoch):
    if epoch <= 2:     
        lr = 5e-4
    elif epoch > 2 and epoch <= 5:
        lr = 2e-4
    elif epoch > 5 and epoch <= 9:
        lr = 5e-5
    else: 
        lr = 1e-5
    return lr

lr_scheduler = LearningRateScheduler(lr_schedule)
checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, 
                               save_best_only=True)

# train the model
hist = model.fit(x_train, y_train, batch_size=32, epochs=20,
          validation_data=(x_valid, y_valid), callbacks=[checkpointer, lr_scheduler], 
          verbose=2, shuffle=True)

# load the best weights
model.load_weights('model.weights.best.hdf5')

"""## Calculate accuracy on the validation set"""

# evaluate test accuracy
score = model.evaluate(x_valid, y_valid, verbose=0)
accuracy = 100*score[1]

# print test accuracy
print('Validation accuracy: %.4f%%' % accuracy)

"""## Evaluate the model"""

# plot train and validation accuracy per epoch
f, ax = plt.subplots()
ax.plot([None] + hist.history['accuracy'], 'o-')
ax.plot([None] + hist.history['val_accuracy'], 'x-')
# Plot legend and use the best location automatically: loc = 0.
ax.legend(['Train acc', 'Validation acc'], loc = 0)
ax.set_title('Training and Validation Accuracy per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('Accuracy')
plt.show()

# plot train and validation loss per epoch
f, ax = plt.subplots()
ax.plot([None] + hist.history['loss'], 'o-')
ax.plot([None] + hist.history['val_loss'], 'x-')

# Plot legend and use the best location automatically: loc = 0.
ax.legend(['Train loss', "Val loss"], loc = 0)
ax.set_title('Training and Validation Loss per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')
plt.show()

"""## Predict on test set and submit """

y_preds = model.predict(x_test)

# argmax is used to get max values.
preds = []
for i in range(len(y_preds)):
    preds.append(np.argmax(y_preds[i]))

ss.Label = preds
ss.head()

ss.to_csv('submission.csv',index=False)

"""<b>The LeNet-5 architecture scores 98.8% validation accuracy."""

